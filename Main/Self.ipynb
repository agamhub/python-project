{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['INSURANCE_CONTRACT_GROUP_plai-iac.csv', 'IAS_SIDELINE_RECORDS.xlsx', 'SP', 'file_name.txt', 'INSURANCE_CONTRACT_GROUP_plai-ias.csv']\n",
      "HEADER1|HEADER2|HEADER3\n",
      "A|B|C\n",
      "B|C|D\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "directory = \"/home/user/python-project/File\"\n",
    "files = os.listdir(directory)\n",
    "\n",
    "print(files)\n",
    "\n",
    "with open(\"/home/user/python-project/File/file_name.txt\", \"r\") as f:\n",
    "    contents = f.read()\n",
    "    print(contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'asdf1234': {'firstname': 'john', 'lastname': 'smith', 'age': 30}, 'sdfg2345': {'firstname': 'jane', 'lastname': 'doe', 'age': 25}, 'dfgh3456': {'firstname': 'billy', 'lastname': 'ericson', 'age': 35}}\n"
     ]
    }
   ],
   "source": [
    "dicts = [\n",
    "   {\n",
    "      \"id\":\"asdf1234\",\n",
    "      \"firstname\":\"john\",\n",
    "      \"lastname\":\"smith\",\n",
    "      \"age\":30\n",
    "   },\n",
    "   {\n",
    "      \"id\":\"sdfg2345\",\n",
    "      \"firstname\":\"jane\",\n",
    "      \"lastname\":\"doe\",\n",
    "      \"age\":25\n",
    "   },\n",
    "   {\n",
    "      \"id\":\"dfgh3456\",\n",
    "      \"firstname\":\"billy\",\n",
    "      \"lastname\":\"ericson\",\n",
    "      \"age\":35\n",
    "   }\n",
    "]    \n",
    "\n",
    "new_dicts = {d.pop('id'): d for d in dicts}\n",
    "\n",
    "print(new_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ENTITY_ID REPORTING_DT           INSURANCE_CONTRACT_GROUP_ID  \\\n",
      "0     IAS_D   2023-12-31        IAI_PRSHAULIDR_VFA_2020_23_VFA   \n",
      "1     IAS_A   2023-12-31        IAI_IVSHAULIDR_VFA_2020_64_VFA   \n",
      "2     IAS_D   2023-12-31      IAI_PRSHAYRTIDR_PAA_2020_344_PAA   \n",
      "3     IAS_A   2023-12-31  IAI_PRSHANULIDR_GMM_2023_334_GMM_NOV   \n",
      "4     IAS_A   2023-12-31        IAI_IVSHAULIDR_VFA_2021_66_VFA   \n",
      "5     IAS_D   2023-12-31             IAI_PRPSURSHANUL_2023_DEC   \n",
      "6     IAS_A   2023-12-31    IAI_IVSHAULIDR_GMM_2009_83_GMM_SUB   \n",
      "7     IAS_A   2023-12-31     IAI_PRSHAULIDR_GMM_2023_2_GMM_MAR   \n",
      "8     IAS_A   2023-12-31    IAI_PRSHAULIDR_GMM_2023_46_GMM_FEB   \n",
      "9     IAS_D   2023-12-31    IAI_PRSHAULIDR_VFA_2014_12_VFA_SUB   \n",
      "\n",
      "  INSURANCE_CONTRACT_GROUP_CD INSURANCE_CONTRACT_PRTFL_ID WORKGROUP  \\\n",
      "0                     REGULAR                  PRSHAULIDR     IAI_D   \n",
      "1                     REGULAR                  IVSHAULIDR     IAI_A   \n",
      "2                     REGULAR                 PRSHAYRTIDR     IAI_D   \n",
      "3                     REGULAR                 PRSHANULIDR     IAI_A   \n",
      "4                     REGULAR                  IVSHAULIDR     IAI_A   \n",
      "5                     REGULAR                PRPSURSHANUL     IAI_D   \n",
      "6                     REGULAR                  IVSHAULIDR     IAI_A   \n",
      "7                     REGULAR                  PRSHAULIDR     IAI_A   \n",
      "8                     REGULAR                  PRSHAULIDR     IAI_A   \n",
      "9                     REGULAR                  PRSHAULIDR     IAI_D   \n",
      "\n",
      "   PRODUCT_LINE_ID REGLTRY_GROUP_CLASS_CD COUNTRY_CD  NO_CONTRACTS  ...  \\\n",
      "0              NaN              REMAINING         ID           NaN  ...   \n",
      "1              NaN            NOT_ONEROUS         ID           NaN  ...   \n",
      "2              NaN              REMAINING         ID           NaN  ...   \n",
      "3              NaN            NOT_ONEROUS         ID           NaN  ...   \n",
      "4              NaN            NOT_ONEROUS         ID           NaN  ...   \n",
      "5              NaN            NOT_ONEROUS         ID           NaN  ...   \n",
      "6              NaN                ONEROUS         ID           NaN  ...   \n",
      "7              NaN            NOT_ONEROUS         ID           NaN  ...   \n",
      "8              NaN            NOT_ONEROUS         ID           NaN  ...   \n",
      "9              NaN            NOT_ONEROUS         ID           NaN  ...   \n",
      "\n",
      "  TRANSITION_LIC_RA_AMT TRANSITION_SDW_CSM_AMT TRANSITION_LREC_AMT  \\\n",
      "0                   0.0                    0.0                 0.0   \n",
      "1                   0.0                    0.0                 0.0   \n",
      "2                   0.0                    0.0                 0.0   \n",
      "3                   0.0                    0.0                 0.0   \n",
      "4                   0.0                    0.0                 0.0   \n",
      "5                   0.0                    0.0                 0.0   \n",
      "6                   0.0                    0.0                 0.0   \n",
      "7                   0.0                    0.0                 0.0   \n",
      "8                   0.0                    0.0                 0.0   \n",
      "9                   0.0                    0.0                 0.0   \n",
      "\n",
      "   TRANSITION_PCA_AMT  TRANSITION_PAA_AMT  TRANSITION_PAA_PREM_AMT  \\\n",
      "0                 0.0                 0.0                      0.0   \n",
      "1                 0.0                 0.0                      0.0   \n",
      "2                 0.0                 0.0                      0.0   \n",
      "3                 0.0                 0.0                      0.0   \n",
      "4                 0.0                 0.0                      0.0   \n",
      "5                 0.0                 0.0                      0.0   \n",
      "6                 0.0                 0.0                      0.0   \n",
      "7                 0.0                 0.0                      0.0   \n",
      "8                 0.0                 0.0                      0.0   \n",
      "9                 0.0                 0.0                      0.0   \n",
      "\n",
      "   TRANSITION_PAA_ACQ_AMT  TRANSITION_PAA_LC_AMT  \\\n",
      "0                     0.0                    0.0   \n",
      "1                     0.0                    0.0   \n",
      "2                     0.0                    0.0   \n",
      "3                     0.0                    0.0   \n",
      "4                     0.0                    0.0   \n",
      "5                     0.0                    0.0   \n",
      "6                     0.0                    0.0   \n",
      "7                     0.0                    0.0   \n",
      "8                     0.0                    0.0   \n",
      "9                     0.0                    0.0   \n",
      "\n",
      "   TRANSITION_ESTATE_BAL_PH_AMT  CURVE_ID  \n",
      "0                           0.0       NaN  \n",
      "1                           0.0       NaN  \n",
      "2                           0.0       NaN  \n",
      "3                           0.0       NaN  \n",
      "4                           0.0       NaN  \n",
      "5                           0.0       NaN  \n",
      "6                           0.0       NaN  \n",
      "7                           0.0       NaN  \n",
      "8                           0.0       NaN  \n",
      "9                           0.0       NaN  \n",
      "\n",
      "[10 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/home/user/python-project/File/INSURANCE_CONTRACT_GROUP_plai-ias.csv', nrows=10)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+------------+------------------+-----------------------------+-----------------+-------------+------------+--------------+--------------+-------------+-----------+--------------------------------------------------+---------+------+------+------+------+------+------+------+------+--------------+----------------+-------------+---------------------------+\n",
      "|    | ENTITY_ID   |    ACCT_CD |   EXTRACT_PERIOD | TXN_DT                      |   AMT_LOCAL_CCY | CR_DR_FLG   |   JRNAL_NO |   JRNAL_LINE | JRNAL_TYPE   | JRNAL_SRC   | TXN_REF   | TXN_DESC                                         |      T0 |   T1 |   T2 |   T3 |   T4 |   T5 | T6   | T7   |   T8 |   JOB_RUN_ID |   BATCH_RUN_ID | VALID_FLG   | FIND_SPECIFIC_CHARACTER   |\n",
      "|----+-------------+------------+------------------+-----------------------------+-----------------+-------------+------------+--------------+--------------+-------------+-----------+--------------------------------------------------+---------+------+------+------+------+------+------+------+------+--------------+----------------+-------------+---------------------------|\n",
      "|  3 | IAC         | 3373000120 |          2024001 | 2024-01-05 00:00:00.0000000 |          247500 | D           |       2816 |          242 | ADJ4         | DWI         | ACCR-0124 | 381723_Dinner Reward Kontes OctoberFest Region J | 0000000 |  nan |  nan |  nan |  nan |  nan | A24  | P01  |  nan |        24134 |           7484 | Y           | True                      |\n",
      "+----+-------------+------------+------------------+-----------------------------+-----------------+-------------+------------+--------------+--------------+-------------+-----------+--------------------------------------------------+---------+------+------+------+------+------+------+------+------+--------------+----------------+-------------+---------------------------+\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "import io\n",
    "from tabulate import tabulate\n",
    "\n",
    "bucket_name = \"pruidlife-nprd-dev-c5wvjk-asia-southeast2-datalake\"\n",
    "file_name = \"data-manual/Renova/ABST_ID_ABST_IFRS4_SUNGL_EXTRACT_IAC_202401.csv\"\n",
    "\n",
    "def read_file_from_gcs(bucket_name, file_name):\n",
    "    \"\"\"Reads a file from a Google Cloud Storage bucket.\"\"\"\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "    # file_name = \"your-file-name\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(file_name)\n",
    "    # Read the file contents\n",
    "    contents = blob.download_as_bytes() # can either use string or bytes\n",
    "    blob_file = io.BytesIO(contents)\n",
    "    data_frame(blob_file)\n",
    "    #print(blob)\n",
    "    #Print the file contents\n",
    "    #print(contents.decode(\"utf-8\"))\n",
    "\n",
    "def data_frame(contents):\n",
    "    #df = pd.read_csv(io.BytesIO(contents))\n",
    "    #df = pd.read_csv(io.BytesIO(contents), nrows=2) to limit rows using nrows\n",
    "    df = pd.read_csv(contents, delimiter='|')\n",
    "    df['FIND_SPECIFIC_CHARACTER'] = df['TXN_DESC'].str.contains('\"')\n",
    "    df = df[df['FIND_SPECIFIC_CHARACTER'] == True]\n",
    "    df['TXN_DESC'] = df['TXN_DESC'].str.replace('\"', '')\n",
    "\n",
    "    print(tabulate(df, headers='keys', tablefmt='psql'))\n",
    "\n",
    "read_file_from_gcs(bucket_name, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pruidlife-nprd-dev-c5wvjk-asia-southeast2-datalake+data-movement SrcFiles/CP/CONFIG/LAND_${BATCHDATE}-ETL1-CONFIG-CONFIG-IFRS17_UC_PROD_ICG_MAP\n"
     ]
    }
   ],
   "source": [
    "# split variable to 2 strings\n",
    "def test_function(location):\n",
    "    bucketname,key = location.split('/',2)[-1].split('/',1) if location.startswith('gs') else location\n",
    "    print(bucketname,key)\n",
    "\n",
    "test_function(\"gs://pruidlife-nprd-dev-c5wvjk-asia-southeast2-datalake/data-movement/SrcFiles/CP/CONFIG/LAND_${BATCHDATE}-ETL1-CONFIG-CONFIG-IFRS17_UC_PROD_ICG_MAP\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ENTITY_ID     INSURANCE_CONTRACT_GROUP_ID  CURVE_ID\n",
      "0     IAC_D  IAI_IVCONULIDR_VFA_2007_57_VFA       NaN\n",
      "  ENTITY_ID     INSURANCE_CONTRACT_GROUP_ID  CURVE_ID\n",
      "0     IAS_D  IAI_PRSHAULIDR_VFA_2020_23_VFA       NaN\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7464\n",
      "  ENTITY_ID     INSURANCE_CONTRACT_GROUP_ID  CURVE_ID\n",
      "0     IAC_D  IAI_IVCONULIDR_VFA_2007_57_VFA       NaN\n",
      "1704\n",
      "  ENTITY_ID     INSURANCE_CONTRACT_GROUP_ID  CURVE_ID\n",
      "0     IAS_D  IAI_PRSHAULIDR_VFA_2020_23_VFA       NaN\n",
      "9168\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/home/user/python-project/File/INSURANCE_CONTRACT_GROUP_plai-iac.csv')\n",
    "#print(df.columns)\n",
    "df2 = len(df)\n",
    "\n",
    "print(df2)\n",
    "df = df[['ENTITY_ID','INSURANCE_CONTRACT_GROUP_ID','CURVE_ID']]\n",
    "df = df.drop_duplicates(subset='CURVE_ID')\n",
    "\n",
    "print(df)\n",
    "\n",
    "df1 = pd.read_csv('/home/user/python-project/File/INSURANCE_CONTRACT_GROUP_plai-ias.csv')\n",
    "#print(df.columns)\n",
    "df3 = len(df1)\n",
    "print(df3)\n",
    "df1 = df1[['ENTITY_ID','INSURANCE_CONTRACT_GROUP_ID','CURVE_ID']]\n",
    "df1 = df1.drop_duplicates(subset='CURVE_ID')\n",
    "\n",
    "#print(df)\n",
    "#df = df.groupby(['ENTITY_ID','INSURANCE_CONTRACT_GROUP_ID','CURVE_ID']).size().to_frame(name = 'count')\n",
    "\n",
    "print(df1)\n",
    "print (df2+df3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'agam', 'age': 20, 'city': 'New York'}, {'name': 'adhi', 'age': 21, 'city': 'New York'}]\n"
     ]
    }
   ],
   "source": [
    "people = [{\"name\":\"agam\",\"age\":20,\"city\":\"\"},{\"name\":\"adhi\",\"age\":21}]\n",
    "# Add a new property called \"city\" to each object\n",
    "for person in people:\n",
    "  person[\"city\"] = \"New York\"\n",
    "print(people)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "directory = \"/home/user/python-project/File/SP/\"\n",
    "file = \"USP_LOAD_ETL5_NOP_NB_APE_DRIVER_DETAIL.sql\"\n",
    "file1 = \"USP_LOAD_ETL5_NOP_NB_APE_DRIVER_DETAIL_utf8.sql\"\n",
    "\n",
    "\n",
    "with open(os.path.join(directory, file), \"r\",encoding=\"latin-1\") as f:\n",
    "        contents = f.read()\n",
    "        print(contents)\n",
    "\n",
    "with open(os.path.join(directory, file1), \"w\",encoding=\"utf-8\") as f:\n",
    "        f.write(contents)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: (40615, b\"Cannot open server 'sdb-sgrass-dev-az1-sql001' requested by the login. Client with IP address '34.80.195.207' is not allowed to access the server.  To enable access, use the Azure Management Portal or run sp_set_firewall_rule on the master database to create a firewall rule for this IP address or address range.  It may take up to five minutes for this change to take effect.DB-Lib error message 20018, severity 14:\\nGeneral SQL Server error: Check messages from the SQL Server\\nDB-Lib error message 20002, severity 9:\\nAdaptive Server connection failed (sdb-sgrass-dev-az1-sql001.database.windows.net)\\nDB-Lib error message 20002, severity 9:\\nAdaptive Server connection failed (sdb-sgrass-dev-az1-sql001.database.windows.net)\\n\")\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'conn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Close the connection\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mconn\u001b[49m:\n\u001b[1;32m     27\u001b[0m         conn\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'conn' is not defined"
     ]
    }
   ],
   "source": [
    "import pymssql\n",
    "\n",
    "# Connection details\n",
    "server = 'sdb-sgrass-dev-az1-sql001.database.windows.net'\n",
    "user = 'ifrs17_idsit_devprod'\n",
    "password = 'CNNJgW8+NFQ='\n",
    "database = 'ifrs17_dw_dev_plai'\n",
    "\n",
    "try:\n",
    "    # Establish the connection\n",
    "    conn = pymssql.connect(server, user, password, database)\n",
    "\n",
    "    # Create a cursor object\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Execute a sample query\n",
    "    cursor.execute(\"SELECT @@version;\")\n",
    "    row = cursor.fetchone()\n",
    "    print(row)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Close the connection\n",
    "    if conn:\n",
    "        conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed SQL:\n",
      "\n",
      "    \n",
      "    \n",
      "    \n",
      "    EXEC FOND_ID.USP_CUSTOM_DATA_QUALITY_CHECK 'ETL4_TOFOND_BATCH02','PKG_PRC_FOND_PLAI_ETL4_LIFEASIA_PARALLEL_STAG','@IS_PRE_CHECK=Y|@PERIOD=202312';\n",
      "  \n",
      "\n",
      "Processed SQL:\n",
      "\n",
      "  UPDATE table2 SET column1 = 'value1'; \n",
      "  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def process_sql(sql_code):\n",
    "  \"\"\"Processes SQL code, skipping green highlighted comments.\"\"\"\n",
    "\n",
    "  # Define a regex pattern to match comments (adjust if needed)\n",
    "  comment_pattern = r\"--.*$\"\n",
    "\n",
    "  # Split the SQL code into lines\n",
    "  lines = sql_code.splitlines()\n",
    "\n",
    "  processed_code = \"\"\n",
    "  for line in lines:\n",
    "    # Remove comments from the current line\n",
    "    stripped_line = re.sub(comment_pattern, \"\", line)\n",
    "    processed_code += stripped_line + \"\\n\"\n",
    "\n",
    "  return processed_code\n",
    "\n",
    "\n",
    "# Example usage in a loop\n",
    "sql_statements = [\n",
    "  \"\"\"\n",
    "    -- DQC in PL_PLAI_TOFOND_ETL4_ETL4_LIFEASIA_STEP1_NEW\n",
    "    -- before running DQC required ABC framework started\n",
    "    -- G_TABLE = GTRN T_TASBLE = ACMV + RTRN\n",
    "    EXEC FOND_ID.USP_CUSTOM_DATA_QUALITY_CHECK 'ETL4_TOFOND_BATCH02','PKG_PRC_FOND_PLAI_ETL4_LIFEASIA_PARALLEL_STAG','@IS_PRE_CHECK=Y|@PERIOD=202312';\n",
    "  \"\"\",\n",
    "  \"\"\"\n",
    "  UPDATE table2 SET column1 = 'value1'; -- Another comment\n",
    "  \"\"\",\n",
    "]\n",
    "\n",
    "for sql in sql_statements:\n",
    "  processed_sql = process_sql(sql)\n",
    "  print(f\"Processed SQL:\\n{processed_sql}\") \n",
    "  # Execute or further analyze the processed_sql here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "include <iostream> \n",
      "\n",
      "int main (){\n",
      "    cout << \"hello world\" << std::endl; \n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyparsing\n",
    "\n",
    "test = \"\"\"\n",
    "/* Code my code\n",
    "xx to remove comments in C++\n",
    "or C or python */\n",
    "\n",
    "include <iostream> // Some comment\n",
    "\n",
    "int main (){\n",
    "    cout << \"hello world\" << std::endl; // comment\n",
    "}\n",
    "\"\"\"\n",
    "commentFilter = pyparsing.cppStyleComment.suppress()\n",
    "# To filter python style comment, use\n",
    "# commentFilter = pyparsing.pythonStyleComment.suppress()\n",
    "# To filter C style comment, use\n",
    "# commentFilter = pyparsing.cStyleComment.suppress()\n",
    "\n",
    "newtest = commentFilter.transformString(test)\n",
    "print(newtest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
